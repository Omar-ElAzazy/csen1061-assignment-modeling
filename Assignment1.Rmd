---
title: "Assignment1"
author: "Omar ElAzazy, 25-1136"
output: html_document
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyr)
library(corrplot)
library(ggplot2)
library(RWeka)
```

```{r echo=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE)
```

#Part 1

Reading the sonar data
```{r}
sonar_data <- read.csv(file="~/Desktop/Tools/csen1061-assignment-modeling/datasets/sonar.all-data.csv", sep=",")
```

#Part 2

Some helper functions
```{r}
print_all_metrics <- function(summary){
  TP <- summary$confusionMatrix[1,1]
  FP <- summary$confusionMatrix[2, 1]
  FN <- summary$confusionMatrix[1, 2]
  TN <- summary$confusionMatrix[2, 2]
  mean_absolute_error <- summary$details["meanAbsoluteError"]
  
  accuracy <- (TP + TN) / (TP + FP + TN + FN)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  f1_score <- 2 * precision * recall / (precision + recall)
  
  list(mean_absolute_error = mean_absolute_error, accuracy = accuracy, precision = precision, recall = recall, f1_score = f1_score) %>% print()
}

get_all_metrics <- function(summary){
  TP <- summary$confusionMatrix[1,1]
  FP <- summary$confusionMatrix[2, 1]
  FN <- summary$confusionMatrix[1, 2]
  TN <- summary$confusionMatrix[2, 2]
  mean_absolute_error <- summary$details["meanAbsoluteError"]
  
  accuracy <- (TP + TN) / (TP + FP + TN + FN)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  f1_score <- 2 * precision * recall / (precision + recall)
  
  list(mean_absolute_error = mean_absolute_error, accuracy = accuracy, precision = precision, recall = recall, f1_score = f1_score)
}
get_metric <- function(summary, name){
  metrics <- summary %>% get_all_metrics()
  metrics[name]
}

apply_all_data_set <- function(model){
  model %>% summary() %>% get_all_metrics
}

apply_10_fold <- function(model, fun){
  model %>% evaluate_Weka_classifier(numFolds = 10) %>% fun
}
```


Using the C4.5 decision tree classifier and tried two methods for training/testing. The first is to train the classifier on the whole data then test it. The other method is to use the 10-fold method. The results show that using the first method, the model was over fitted to the data thus the average error was minimal compared to the second method. While in the second method since the classifier is trained on k-1 parts of the data then tested on the missing part, the data is not overfitted which is shown in the higher average error.
```{r}
c4.5 <- J48(R~., data=sonar_data)
c4.5 %>% apply_all_data_set()
c4.5 %>% apply_10_fold(print_all_metrics)
```

#Part 3

Tried different classifiers.


Random Forest:


In this one, the f1 score is higher than that of the C4.5 decision tree. Which shows that using a random forest here could be more suitable.
```{r}
random_forest <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
random_forest(R~., data=sonar_data) %>% apply_10_fold(print_all_metrics)
```

Support vector machines


In this classifier, the f1 score was less than that of the random forest, but it has less average error.
```{r}
svm <- SMO(R~., data=sonar_data)
svm %>% apply_10_fold(print_all_metrics)
```

Naive Bayes

In this classifier, the f1 score decreased even more than that of the C4.5 decision tree so this classifier looks the worst to use for this data.
```{r}
naive_bayes <- make_Weka_classifier("weka/classifiers/bayes/NaiveBayes")
naive_bayes(R~., data=sonar_data) %>% apply_10_fold(print_all_metrics)
```

Neural Networks

This classifier had the highest f1 score with the least average error.
```{r}
neural_network <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
neural_network(R~., data=sonar_data) %>% apply_10_fold(print_all_metrics)
```

Bagging
```{r}
bagging <- Bagging(R~., data=sonar_data, control = Weka_control(W=list(J48,M=30)))
bagging %>% apply_10_fold(print_all_metrics)
```

Adaptive Boosting
```{r}
boosting <- AdaBoostM1(R~., data=sonar_data, control=Weka_control(W=list(J48,"--",M=30)))
boosting %>% apply_10_fold(print_all_metrics)
```


Both bagging and adaptive boosting had similar f1 scores, which are both higher than using a C4.5 decision tree and not an ensemble classifier with the base as the C4.5 decision tree. This shows the use of ensemble classifiers.
#Part 3

Hepatitis
```{r}
hepatitis <- read.csv(file="~/Desktop/Tools/csen1061-assignment-modeling/datasets/hepatitis.data.csv", sep=",")
```

Spect
```{r}
spect <- read.csv(file="~/Desktop/Tools/csen1061-assignment-modeling/datasets/SPECT.csv", sep=",")
```

Pima-indians
```{r}
pima <- read.csv(file="~/Desktop/Tools/csen1061-assignment-modeling/datasets/pima-indians-diabetes.data.csv", sep=",")
```

```{r}
apply_10_fold_10_times <- function(model, metric){
  m <- replicate(10, replicatemodel %>% apply_10_fold(get_metric(metric)))
  mean(m)
}

get_models <- function(data, label){
  c4.5 <- J48(label, data=data)
  
  random_forest <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
  random_forest <- random_forest(label, data=data)
  
  svm <- SMO(label, data=data)
  
  naive_bayes <- make_Weka_classifier("weka/classifiers/bayes/NaiveBayes")
  naive_bayes <- naive_bayes(label, data=data)
  
  neural_network <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
  neural_network <- neural_network(label, data=data)
  
  bagging <- Bagging(label, data=data, control = Weka_control(W=list(J48,M=30)))
  
  boosting <- AdaBoostM1(label, data=data, control=Weka_control(W=list(J48,"--",M=30)))
  
  list(c4.5 = c4.5, random_forest = random_forest, svm = svm, naive_bayes = naive_bayes, neural_network = neural_network, bagging = bagging, boosting = boosting)
}

get_tables <- function(){
  data <- c(sonar_data = sonar_data, hepatitis = hepatitis, spect = spect, pima = pima)
  labels <- c("R", "X1.2", "X0.11", "X1")
  metrics <- c("mean_absolute_error", "accuracy", "precision", "recall", "f1_score")
  data %>% sapply(function(x) {
     #%>% sapply()
  })
}
```

  
